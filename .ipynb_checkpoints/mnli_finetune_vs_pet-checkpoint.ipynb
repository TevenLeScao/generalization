{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQn13N4Ym6t6"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "I9hoPxLIm-eI",
    "outputId": "7997afbf-9091-44de-8e78-d8efdfb49292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow==0.16.0 in /usr/local/lib/python3.6/dist-packages (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow==0.16.0) (1.18.5)\n",
      "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow==0.16.0) (1.15.0)\n",
      "Requirement already satisfied: nlp in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.1.1)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp) (1.0.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from nlp) (2.0.0)\n",
      "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow>=0.16.0->nlp) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2.8.1)\n",
      "Requirement already satisfied: apache-beam in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from apache-beam) (1.9.2.1)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (3.0.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (2.5.8)\n",
      "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (0.17.4)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (2018.9)\n",
      "Requirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (1.18.5)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (2.0.0)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (3.11.0)\n",
      "Requirement already satisfied: pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from apache-beam) (0.16.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (1.31.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.5.0.post1 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (3.7.4.2)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (1.7)\n",
      "Requirement already satisfied: fastavro<0.24,>=0.21.4 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (0.23.6)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (1.3.0)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (0.3.1.1)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.6/dist-packages (from apache-beam) (0.18.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam) (4.6)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam) (0.4.8)\n",
      "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam) (0.2.8)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (0.6.2)\n",
      "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (2.23.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam) (5.4.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf<4,>=3.5.0.post1->apache-beam) (49.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot<2,>=1.2.0->apache-beam) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow==0.16.0\n",
    "!pip install nlp\n",
    "!pip install apache-beam\n",
    "\n",
    "# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\n",
    "import pyarrow\n",
    "if int(pyarrow.__version__.split('.')[0]) == 0 and int(pyarrow.__version__.split('.')[1]) < 16:\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)\n",
    "\n",
    "import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "CumcFXslm_nF",
    "outputId": "a20de957-a21a-4c95-9205-424f9b269224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, RobertaForMaskedLM, RobertaTokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XJPHRSuwnEDK",
    "outputId": "8766ff62-d43b-45be-a81b-17b62edc96db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print(\"Using CUDA!\")\n",
    "    torch_t = torch.cuda\n",
    "    def from_numpy(ndarray):\n",
    "        return torch.from_numpy(ndarray).pin_memory().cuda(async=True)\n",
    "else:\n",
    "    print(\"Not using CUDA!\")\n",
    "    torch_t = torch\n",
    "    from torch import from_numpy\n",
    "    \n",
    "def torch_load(load_path):\n",
    "    if use_cuda:\n",
    "        return torch.load(load_path)\n",
    "    else:\n",
    "        return torch.load(load_path, map_location = \n",
    "                          lambda storage, location: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "Pc3z-J5knIsq",
    "outputId": "922431ae-665f-4456-be71-cab1f6250781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 25 17:59:38 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   50C    P0    41W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYQqtbbPnL1X"
   },
   "outputs": [],
   "source": [
    "class SentenceOrWordBert(nn.Module):\n",
    "    def __init__(self, model_type, model_name):\n",
    "        super().__init__()\n",
    "        MODEL_CLASSES = {\n",
    "            'bert': (BertForMaskedLM, BertTokenizer),\n",
    "            'roberta': (RobertaForMaskedLM, RobertaTokenizer),\n",
    "        }\n",
    "        model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(model_name, do_lower_case=('uncased' in model_name))\n",
    "        self.bert_lm = model_class.from_pretrained(model_name)\n",
    "        self.bert = self.bert_lm.bert\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.dim = self.bert.pooler.dense.in_features\n",
    "        self.max_len = self.bert.embeddings.position_embeddings.num_embeddings\n",
    "        \n",
    "        self.unmasked_label_id = -100\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "    \n",
    "    def tokenize(self, sent, label_mask, include_clssep = False):\n",
    "        \"\"\"\n",
    "        sent - string or list of words\n",
    "        label_mask - list of either 0 or 1, 1 for masked (same length as sent)\n",
    "        include_clssep - whether or not to include [CLS] and [SEP] in end_mask\n",
    "        \n",
    "        input_ids - sent converted to ids, with some ids masked\n",
    "        end_mask - each word might be multiple subwords, so end_mask is 1 for\n",
    "            the final subword of each word\n",
    "        label_ids - -100 for unmasked tokens, the label for masked tokens\n",
    "        \"\"\"\n",
    "        if isinstance(sent, str):\n",
    "            sent = word_tokenize(sent)\n",
    "        if label_mask is None:\n",
    "            label_mask = [0 for _ in sent]\n",
    "        \n",
    "        input_ids = [self.tokenizer.cls_token_id]\n",
    "        end_mask = [int(include_clssep)]\n",
    "        label_ids = [self.unmasked_label_id]\n",
    "        for word, is_masked in zip(sent, label_mask):\n",
    "            ids = self.tokenizer.encode(word, add_special_tokens = False)\n",
    "            assert len(ids) > 0, \"Unknown word {} in {}\".format(word, sent)\n",
    "            if is_masked:\n",
    "                input_ids.extend([self.tokenizer.mask_token_id for _ in ids])\n",
    "                label_ids.extend(ids)\n",
    "            else:\n",
    "                input_ids.extend(ids)\n",
    "                label_ids.extend([self.unmasked_label_id for _ in ids])\n",
    "            end_mask.extend([0 for _ in ids])\n",
    "            end_mask[-1] = 1\n",
    "        input_ids.append(self.tokenizer.sep_token_id)\n",
    "        end_mask.append(int(include_clssep))\n",
    "        label_ids.append(self.unmasked_label_id)\n",
    "        return input_ids, end_mask, label_ids\n",
    "    \n",
    "    def tokenize_sentences(self, sentences, include_clssep = False, label_masks = None):\n",
    "        \"\"\"\n",
    "        sentences - list of sentences, or tuples containing 2 sentences each\n",
    "            each sentence is either a string a list of words\n",
    "        \"\"\"\n",
    "        paired = isinstance(sentences[0], tuple) and len(sentences[0]) == 2\n",
    "        if label_masks is None:\n",
    "            if paired:\n",
    "                label_masks = [(None, None) for _ in sentences]\n",
    "            else:\n",
    "                label_masks = [None for _ in sentences]\n",
    "            \n",
    "        all_input_ids = np.zeros((len(sentences), self.max_len), dtype = int) + self.tokenizer.pad_token_id\n",
    "        all_input_mask = np.zeros((len(sentences), self.max_len), dtype = int)\n",
    "        all_end_mask = np.zeros((len(sentences), self.max_len), dtype = int)\n",
    "        all_label_ids = np.zeros((len(sentences), self.max_len), dtype = int) + self.unmasked_label_id\n",
    "        \n",
    "        max_sent = 0\n",
    "        for s_num, (sent, label_mask) in enumerate(zip(sentences, label_masks)):\n",
    "            if paired:\n",
    "                input_ids1, end_mask1, label_ids1 = self.tokenize(sent[0], label_mask[0], include_clssep = include_clssep)\n",
    "                input_ids2, end_mask2, label_ids2 = self.tokenize(sent[1], label_mask[1], include_clssep = include_clssep)\n",
    "    \n",
    "                input_ids = input_ids1 + input_ids2[1:] # [cls] sent1 [sep] sent2 [sep]\n",
    "                end_mask = end_mask1 + end_mask2[1:]\n",
    "                label_ids = label_ids1 + label_ids2[1:]\n",
    "            else:\n",
    "                input_ids, end_mask, label_ids = self.tokenize(\n",
    "                    sent, label_mask, include_clssep = include_clssep)\n",
    "\n",
    "            all_input_ids[s_num, :len(input_ids)] = input_ids\n",
    "            all_input_mask[s_num, :len(input_ids)] = 1\n",
    "            all_end_mask[s_num, :len(input_ids)] = end_mask\n",
    "            all_label_ids[s_num, :len(input_ids)] = label_ids\n",
    "            max_sent = max(max_sent, len(input_ids))\n",
    "            \n",
    "        all_input_ids = from_numpy(np.ascontiguousarray(all_input_ids[:, :max_sent]))\n",
    "        all_input_mask = from_numpy(np.ascontiguousarray(all_input_mask[:, :max_sent]))\n",
    "        all_end_mask = from_numpy(np.ascontiguousarray(all_end_mask[:, :max_sent])).to(torch.uint8)\n",
    "        all_label_ids = from_numpy(np.ascontiguousarray(all_label_ids[:, :max_sent]))\n",
    "        return all_input_ids, all_input_mask, all_end_mask, all_label_ids\n",
    "\n",
    "    def run_bert(self, all_input_ids, all_input_mask, subbatch_size = 64):\n",
    "        \"\"\"\n",
    "        all_input_ids, all_input_mask - tensors (batch, maxlen)\n",
    "        features_all - tensor (batch, maxlen, dim)\n",
    "        hidden_all - tuple of tensors (batch, maxlen, dim), one for embedding and one for each layer\n",
    "        attentions_all - tuple of tensors (batch, heads, maxlen, maxlen), one for each layer\n",
    "        \"\"\"\n",
    "        features_all = None\n",
    "        attentions_all = None\n",
    "        for i in range(0, len(all_input_ids), subbatch_size):\n",
    "            input_ids = all_input_ids[i:i+subbatch_size]\n",
    "            input_mask = all_input_mask[i:i+subbatch_size]\n",
    "            \n",
    "            # (batch, len, dim or vocab_size), tuple((batch, heads, len, len) x layers)\n",
    "            features, _, attentions = self.bert(input_ids, attention_mask = input_mask, output_attentions = True)\n",
    "                \n",
    "            if features_all is None:\n",
    "                features_all = features\n",
    "                attentions_all = list(attentions)\n",
    "            else:\n",
    "                features_all = torch.cat((features_all, features), dim = 0)\n",
    "                for i, (attn_all, attn) in enumerate(zip(attentions_all, attentions)):\n",
    "                    attentions_all[i] = torch.cat((attn_all, attn), dim = 0)\n",
    "        return features_all, attentions_all\n",
    "\n",
    "    def annotate(self, sentences, include_clssep = False, word_level = False, subbatch_size = 64):\n",
    "        \"\"\"\n",
    "        Input: list of sentences or sentence pairs\n",
    "            include_clssep - whether or not to include [CLS] and [SEP] in the end_mask\n",
    "        Output: tensor (len(sentences), bert_dim) with sentence representations, or\n",
    "            tensor (num_words_packed, bert_dim) with word representations\n",
    "        \"\"\"\n",
    "        all_input_ids, all_input_mask, all_end_mask, _ = \\\n",
    "            self.tokenize_sentences(sentences, include_clssep = include_clssep)\n",
    "        features, attn = self.run_bert(all_input_ids, all_input_mask, subbatch_size = subbatch_size)\n",
    "        if word_level:\n",
    "            features = features.masked_select(all_end_mask.unsqueeze(-1)).reshape(-1, features.shape[-1])\n",
    "        else:\n",
    "            features = features[:,0]\n",
    "        return features, all_input_ids, attn\n",
    "    \n",
    "    def run_bert_lm(self, all_input_ids, all_input_mask, all_label_ids, subbatch_size = 64):\n",
    "        \"\"\"\n",
    "        all_input_ids, all_input_mask, all_label_ids - tensors (batch, maxlen)\n",
    "        loss - 0-dim tensor\n",
    "        logits_all - tensor (num_masked, vocabsize)\n",
    "        hidden_all - tuple of tensors (batch, maxlen, dim), one for embedding and one for each layer\n",
    "        attentions_all - tuple of tensors (batch, heads, maxlen, maxlen), one for each layer\n",
    "        \"\"\"        \n",
    "        loss_sum, n = None, 0\n",
    "        logits_all = None\n",
    "        attentions_all = None\n",
    "        for i in range(0, len(all_input_ids), subbatch_size):\n",
    "            input_ids = all_input_ids[i:i+subbatch_size]\n",
    "            input_mask = all_input_mask[i:i+subbatch_size]\n",
    "            label_ids = all_label_ids[i:i+subbatch_size]\n",
    "\n",
    "            loss, logits, attentions = self.bert_lm(input_ids, attention_mask = input_mask, labels = label_ids, output_attentions = True)\n",
    "            logits = logits.masked_select((label_ids != -100).unsqueeze(-1)).reshape(-1, logits.shape[-1])\n",
    "            \n",
    "            if loss_sum is None:\n",
    "                loss_sum, n = loss, 1\n",
    "                logits_all = logits\n",
    "                attentions_all = list(attentions)\n",
    "            else:\n",
    "                loss_sum, n = loss_sum + loss, n + 1\n",
    "                logits_all = torch.cat((logits_all, logits), dim = 0)\n",
    "                for i, (attn_all, attn) in enumerate(zip(attentions_all, attentions)):\n",
    "                    attentions_all[i] = torch.cat((attn_all, attn), dim = 0)\n",
    "        return loss_sum / n, logits_all, attentions_all\n",
    "    \n",
    "    def predict_mlm(self, sentences, label_masks, output_hidden = False, subbatch_size = 64):\n",
    "        \"\"\"\n",
    "        Input: list of sentences, which are lists of words.\n",
    "            list of masks, which are lists of either 0 or 1, 1 for masked\n",
    "        Output: tensor (num_masked, vocab_size) with prediction logits\n",
    "        \"\"\"\n",
    "        all_input_ids, all_input_mask, _, all_label_ids = self.tokenize_sentences(sentences, label_masks = label_masks)\n",
    "        _, logits_all, attn = self.run_bert_lm(all_input_ids, all_input_mask, all_label_ids, subbatch_size = subbatch_size)\n",
    "        return logits_all, all_input_ids, attn\n",
    "\n",
    "    def reset_weights(self, encoder_only = True):\n",
    "        for name, module in self.named_modules():\n",
    "            if hasattr(module, 'reset_parameters') and ('encoder' in name or not encoder_only):\n",
    "                module.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TskJ7_QanOdY"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, bert, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "        self.span_tip = nn.Linear(self.bert.dim, num_classes)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, sentences, word_level = False):\n",
    "        embs, _, _ = self.bert.annotate(sentences, word_level = word_level)\n",
    "        return self.span_tip(self.dropout(embs))\n",
    "    \n",
    "    def freeze_head(self, freeze = True):\n",
    "        for param in self.span_tip.parameters():\n",
    "            param.requires_grad = not freeze\n",
    "    \n",
    "class MLMClassifier(nn.Module):\n",
    "    def __init__(self, bert, classes): # ex: [['he', 'him', 'himself'], ['she', 'her', 'herself']]\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classes = classes\n",
    "        self.class_ids = []\n",
    "        for lst in classes:\n",
    "            ids = [self.bert.tokenizer.convert_tokens_to_ids(word) for word in lst]\n",
    "            assert self.bert.tokenizer.unk_token_id not in ids\n",
    "            self.class_ids.append(ids)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "            \n",
    "    def forward(self, exmps): \n",
    "        # sentences are lists of words, label_masks are lists of 0/1 where 1 = masked\n",
    "        sentences, label_masks = [e[0] for e in exmps], [e[1] for e in exmps]\n",
    "        if isinstance(label_masks[0], tuple):\n",
    "            assert all([sum(mask1 + mask2) == 1 for mask1, mask2 in label_masks]),  \"There should be one masked word per sentence pair\"\n",
    "        else:\n",
    "            assert all([sum(mask) == 1 for mask in label_masks]), \"There should be one masked word per sentence\"\n",
    "        output_logits, _, _ = self.bert.predict_mlm(sentences, label_masks)\n",
    "        assert len(output_logits) == len(sentences), \\\n",
    "            \"Masked words should be one subword, but num_masks {} =/= num_sent {}\".format(len(output_logits), len(sentences))\n",
    "        output_logits = F.log_softmax(output_logits, dim = -1)\n",
    "        class_logits = torch.cat(tuple(torch.logsumexp(output_logits[:, ids], dim = -1).unsqueeze(-1) for ids in self.class_ids), dim = -1)\n",
    "        return class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dRWEOVInSwE"
   },
   "outputs": [],
   "source": [
    "def calc_dev(model, dev_data, subbatch_size = 128): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = None\n",
    "        for j in range(0, len(dev_data), subbatch_size):\n",
    "            examples = dev_data[j:j+subbatch_size]\n",
    "            logits = model([exmp[0] for exmp in examples])\n",
    "            preds_batch = np.argmax(logits.cpu().numpy(), axis = 1)\n",
    "            if preds is None:\n",
    "                preds = preds_batch\n",
    "            else:\n",
    "                preds = np.concatenate((preds, preds_batch), axis = 0)\n",
    "        dev_acc = np.sum(np.array([exmp[1] for exmp in dev_data]) == preds) / len(dev_data)\n",
    "    return dev_acc\n",
    "\n",
    "def train(model, train_data, dev_data, lr_base = 3e-5, lr_warmup_frac = 0.1,\n",
    "          epochs = 5, batch_size = 32, subbatch_size = 8, verbose = True):\n",
    "    print(\"lr_base: {}, lr_warmup_frac: {}, epochs: {}, batch_size: {}, len(train_data): {}\".format(\n",
    "        lr_base, lr_warmup_frac, epochs, batch_size, len(train_data)))\n",
    "\n",
    "    bert_params = [p for n, p in model.named_parameters() if 'mask_score' not in n and p.requires_grad]\n",
    "    trainer = torch.optim.Adam([\n",
    "        {'params': bert_params, 'lr': 0., 'lr_base': lr_base, 'name': 'bert'},], lr = 0.)\n",
    "    \n",
    "    def set_lr(lr_ratio):\n",
    "        for param_group in trainer.param_groups:\n",
    "            param_group['lr'] = param_group['lr_base'] * lr_ratio\n",
    "\n",
    "    log = []\n",
    "    processed = 0\n",
    "    check_processed = 0\n",
    "    check_every = 2048\n",
    "    train_acc_sum, train_acc_n = 0, 0\n",
    "    for epoch in tqdm.notebook.tqdm(range(epochs)):\n",
    "        np.random.shuffle(train_data)\n",
    "        for i in tqdm.notebook.tqdm(range(0, len(train_data), batch_size)):\n",
    "            examples = train_data[i:i+batch_size]\n",
    "            if len(examples) == batch_size:\n",
    "                model.train()\n",
    "                trainer.zero_grad()\n",
    "\n",
    "                for j in range(0, len(examples), subbatch_size):\n",
    "                    examples_subbatch = examples[j:j+subbatch_size]\n",
    "\n",
    "                    # compute loss, also log other metrics\n",
    "                    logits = model([exmp[0] for exmp in examples_subbatch])\n",
    "                    labels = np.array([exmp[1] for exmp in examples_subbatch])\n",
    "                    loss = F.cross_entropy(logits, from_numpy(labels))\n",
    "                    loss.backward()\n",
    "                    bert_grad_norm = torch.nn.utils.clip_grad_norm_(bert_params, np.inf).item()\n",
    "                    loss_val = loss.item()\n",
    "                    del loss\n",
    "\n",
    "                    batch_acc = np.sum(labels == np.argmax(logits.detach().cpu().numpy(), axis = 1)) / len(labels)\n",
    "                    train_acc_sum += batch_acc\n",
    "                    train_acc_n += 1\n",
    "\n",
    "                trainer.step()\n",
    "                processed += len(examples)\n",
    "                check_processed += len(examples)\n",
    "\n",
    "                # warmup from 0 to lr_base for lr_warmup_frac\n",
    "                lr_ratio = min(1, processed / (lr_warmup_frac * epochs * len(train_data)))\n",
    "                set_lr(lr_ratio)\n",
    "\n",
    "                if check_processed >= check_every:\n",
    "                    log.append({'dev_acc': calc_dev(model, dev_data), \n",
    "                                'train_acc': train_acc_sum / train_acc_n, \n",
    "                                'loss_val': loss_val,\n",
    "                                'bert_grad_norm': bert_grad_norm})\n",
    "                    train_acc_sum, train_acc_n = 0, 0\n",
    "                    check_processed -= check_every\n",
    "                    if verbose:\n",
    "                        print(\"Epoch: {}, Log: {}\".format(epoch, log[-1]))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VchDQxgoK2W"
   },
   "outputs": [],
   "source": [
    "def setup_dataset(dataset, num_labels, num_examples, dataset_name):\n",
    "    np.random.seed(0)\n",
    "    idx = np.arange(len(dataset))\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    if dataset_name == 'mnli':\n",
    "        num_per_label = num_examples // num_labels\n",
    "        current_num_per_label = [0 for _ in range(num_labels)]\n",
    "        examples = []\n",
    "        for i in idx:\n",
    "            exmp = dataset[int(i)]\n",
    "            if all([num == num_per_label for num in current_num_per_label]):\n",
    "                break\n",
    "            if current_num_per_label[exmp['label']] < num_per_label:\n",
    "                current_num_per_label[exmp['label']] += 1\n",
    "                examples.append(((exmp['hypothesis'], exmp['premise']), exmp['label']))\n",
    "    elif dataset_name in ('cola', 'sst2'):\n",
    "        num_per_label = num_examples // num_labels\n",
    "        current_num_per_label = [0 for _ in range(num_labels)]\n",
    "        examples = []\n",
    "        for i in idx:\n",
    "            exmp = dataset[int(i)]\n",
    "            if all([num == num_per_label for num in current_num_per_label]):\n",
    "                break\n",
    "            if current_num_per_label[exmp['label']] < num_per_label:\n",
    "                current_num_per_label[exmp['label']] += 1\n",
    "                examples.append((exmp['sentence'], exmp['label']))\n",
    "    else:\n",
    "        assert False, \"Not implemented for dataset {}\".format(dataset_name) \n",
    "    return examples\n",
    "\n",
    "def convert_to_mlm(examples):\n",
    "    classes = [['yes', 'right'], ['maybe'], ['wrong', 'no']]\n",
    "    mlm_examples = []\n",
    "    for (hyp, premise), label in examples:\n",
    "        hyp = word_tokenize(hyp)\n",
    "        premise = [classes[label][0]] + [','] + word_tokenize(premise)\n",
    "        mask1 = [0 for _ in hyp]\n",
    "        mask2 = [0 for _ in premise]\n",
    "        mask2[0] = 1\n",
    "        mlm_examples.append((((hyp,premise), (mask1, mask2)), label))\n",
    "    return mlm_examples, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "O3CY4knUqU5N",
    "outputId": "d29527c0-a4dd-4e43-b859-a7c40c96d2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49998 510\n",
      "(('Before we send this to federal court, there is a conference.', 'Before any legal action is brought against this rule in federal court, the administrative appeal rights set forth at 7 C.F.R.'), 2) (('The plants are strictly seasonal, only grown during the winter.', \"oh really it wouldn't matter if we plant them when it was starting to get warmer\"), 2)\n"
     ]
    }
   ],
   "source": [
    "do_mlm = False\n",
    "\n",
    "dataset = nlp.load_dataset('glue', 'mnli')\n",
    "num_labels = max([exmp['label'] for exmp in dataset['validation_matched']]) + 1\n",
    "train_data = setup_dataset(dataset['train'], num_labels, 10000, 'mnli')\n",
    "dev_data = setup_dataset(dataset['validation_matched'], num_labels, 512, 'mnli')\n",
    "\n",
    "if do_mlm:\n",
    "    train_data, classes = convert_to_mlm(train_data)\n",
    "    dev_data, classes = convert_to_mlm(dev_data)\n",
    "\n",
    "print(len(train_data), len(dev_data))\n",
    "print(train_data[0], dev_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679,
     "referenced_widgets": [
      "ac5cd408258143a29b2d1e3834113d2c",
      "03717b39bb2b48acb220d9a297e9989e",
      "f34ea350845d4887a4cf20b41010f362",
      "a461f398834c4df98c5b388f9b83734f",
      "5374d5af7a7349eca4fdfca1b042b4e8",
      "0cc84282157444b494a372e91410f70e",
      "4c95bbaeb86a4f439180c9383cda2f27",
      "01470d5db625406ab454c4b6c86a66fe",
      "570a0106258d4b2ea861bb76e043fe97",
      "ad1ad773f4214f07b60e9a36b14002ed",
      "1fc6df5e49404198a1e703d015225866",
      "6137ac2dcb77452682111890b43ed8a9",
      "39fd6321a3e94d38ba4c8f8bb7ebdfb7",
      "ca35b3daba1645ccb635052787398f29",
      "6a204b189fd04796b62262f4fb561640",
      "3973433c1b124d86bb030fbf36f4840f"
     ]
    },
    "colab_type": "code",
    "id": "gfjhq41Bqzo5",
    "outputId": "361cf687-f002-4040-8d78-8e0d518a92b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_base: 3e-05, lr_warmup_frac: 0.1, epochs: 5, batch_size: 32, len(train_data): 49998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5cd408258143a29b2d1e3834113d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570a0106258d4b2ea861bb76e043fe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1563.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: This overload of cuda is deprecated:\n",
      "\tcuda(torch.device device, bool async, *, torch.memory_format memory_format)\n",
      "Consider using one of the following signatures instead:\n",
      "\tcuda(torch.device device, bool non_blocking, *, torch.memory_format memory_format) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Log: {'dev_acc': 0.31176470588235294, 'train_acc': 0.33935546875, 'loss_val': 1.1097607612609863, 'bert_grad_norm': 13.136543273925781}\n",
      "Epoch: 0, Log: {'dev_acc': 0.3627450980392157, 'train_acc': 0.35009765625, 'loss_val': 1.0792288780212402, 'bert_grad_norm': 11.144452095031738}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ca9c2b2a2c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final results: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f0e1982f222f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, dev_data, lr_base, lr_warmup_frac, epochs, batch_size, subbatch_size, verbose)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0mbert_grad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwill\u001b[0m \u001b[0maccumulate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0minto\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \"\"\"\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"retains_grad\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n\u001b[1;32m    735\u001b[0m                           \u001b[0;34m\"attribute won't be populated during autograd.backward(). If you indeed want the gradient \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert = SentenceOrWordBert('bert', 'bert-base-uncased')\n",
    "if do_mlm:\n",
    "    model = MLMClassifier(bert, classes)\n",
    "else:\n",
    "    model = MLPClassifier(bert, num_labels)\n",
    "\n",
    "log = train(model, train_data, dev_data, verbose = True, epochs = 5)\n",
    "print(\"Final results: {}\".format(log[-1]))\n",
    "for key in log[0].keys():\n",
    "    plt.plot(np.arange(len(log)), [a[key] for a in log], color='blue')\n",
    "    plt.title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSZYwr0htT6K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mnli-finetune-vs-pet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01470d5db625406ab454c4b6c86a66fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03717b39bb2b48acb220d9a297e9989e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cc84282157444b494a372e91410f70e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fc6df5e49404198a1e703d015225866": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  9%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca35b3daba1645ccb635052787398f29",
      "max": 1563,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39fd6321a3e94d38ba4c8f8bb7ebdfb7",
      "value": 139
     }
    },
    "3973433c1b124d86bb030fbf36f4840f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39fd6321a3e94d38ba4c8f8bb7ebdfb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4c95bbaeb86a4f439180c9383cda2f27": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5374d5af7a7349eca4fdfca1b042b4e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "570a0106258d4b2ea861bb76e043fe97": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1fc6df5e49404198a1e703d015225866",
       "IPY_MODEL_6137ac2dcb77452682111890b43ed8a9"
      ],
      "layout": "IPY_MODEL_ad1ad773f4214f07b60e9a36b14002ed"
     }
    },
    "6137ac2dcb77452682111890b43ed8a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3973433c1b124d86bb030fbf36f4840f",
      "placeholder": "​",
      "style": "IPY_MODEL_6a204b189fd04796b62262f4fb561640",
      "value": " 139/1563 [01:05&lt;09:54,  2.40it/s]"
     }
    },
    "6a204b189fd04796b62262f4fb561640": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a461f398834c4df98c5b388f9b83734f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01470d5db625406ab454c4b6c86a66fe",
      "placeholder": "​",
      "style": "IPY_MODEL_4c95bbaeb86a4f439180c9383cda2f27",
      "value": " 0/5 [01:05&lt;?, ?it/s]"
     }
    },
    "ac5cd408258143a29b2d1e3834113d2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f34ea350845d4887a4cf20b41010f362",
       "IPY_MODEL_a461f398834c4df98c5b388f9b83734f"
      ],
      "layout": "IPY_MODEL_03717b39bb2b48acb220d9a297e9989e"
     }
    },
    "ad1ad773f4214f07b60e9a36b14002ed": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca35b3daba1645ccb635052787398f29": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f34ea350845d4887a4cf20b41010f362": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cc84282157444b494a372e91410f70e",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5374d5af7a7349eca4fdfca1b042b4e8",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
